SET PYTHON BREAKPOINT
-----------------------------------------------------------
Run locally after restart - one in each terminal:
mongod
rabbitmq-server
celery multi start 2 -A text_processing.tasks -l info -P eventlet
celery flower
mvn exec:java -Dexec.mainClass="edu.mit.civic.mediacloud.ParseServer" -Dexec.args="-Xmx2g"

python server.py (from application dir) 

UPDATE BERYL WITH NEW CODE
-----------------------------------------------------------
git fetch --all
git reset --hard origin/master
in /var/www/terra-incognita.co/Terra-Incognita

start/stop apache


SET PYTHON BREAKPOINT
-----------------------------------------------------------
import pdb; pdb.set_trace()

REMOVE PYC FILES
-----------------------------------------------------------
find . -name '*.pyc' -delete

MONGODB 
-----------------------------------------------------------
run: mongod

check if running on beryl:
service mongodb status

restart on beryl:
sudo service mongodb stop
sudo service mongodb restart

MONGODB SHELL
-----------------------------------------------------------
Launch: /usr/local/bin/mongo

IF MONGO HAS PASSWORD
mongo -u username -p password --authenticationDatabase admin

over http://terra-incognita.co:28017/

Drop DB:
use <db name>
db.dropDatabase()

GITHUB HARD RESET
-----------------------------------------------------------
For resetting git working dir
git fetch --all
git reset --hard origin/master

Change permissions to www-data
sudo chgrp www-data /var/www -R

START STOP APACHE
-----------------------------------------------------------
sudo /etc/init.d/apache2 stop
sudo /etc/init.d/apache2 start
sudo /etc/init.d/apache2 restart

START STOP CLIFF-CLAVIN GEOPARSING
-----------------------------------------------------------
mvn exec:java -Dexec.mainClass="edu.mit.civic.mediacloud.ParseServer" -Dexec.args="-Xmx2g"

CLIFF SERVER STUFF
-----------------------------------------------------------
CLIFF on civicdev server runs as an upstart service which should start automatically on machine boot. 

LOCATION: /usr/local/lib/cliff

To start stop:

sudo service cliff start
sudo service cliff start

UPDATING FROM GIT:
Go to /usr/local/lib/cliff
sudo service cliff stop
git fetch --all
git reset --hard origin/master
sudo mvn compile
sudo service cliff start


CELERY
-----------------------------------------------------------
use start.sh/stop.sh scripts
call worker: extract.delay(INPUT)
Start Celery Flower on 5555: celery flower
Celery Flower -- terra-incognita.co:5555/ OR http://127.0.0.1:5555/
Celery flower -- consider how to secure (password, SSL, only on launch)


on BERYL
sudo /etc/init.d/celeryd start/stop/status/restart

RABBIT MQ
-----------------------------------------------------------
Mgmt plugin: http://localhost:15672
/usr/local/Cellar/rabbitmq/3.2.3
To have launchd start rabbitmq at login:
    ln -sfv /usr/local/opt/rabbitmq/*.plist ~/Library/LaunchAgents
Then to load rabbitmq now:
    launchctl load ~/Library/LaunchAgents/homebrew.mxcl.rabbitmq.plist
Or, if you don't want/need launchctl, you can just run:
    rabbitmq-server

Local logging: /usr/local/var/log/rabbitmq/rabbit@localhost.log
Local config: /usr/local/etc/rabbitmq

QUERIES FOR MONGO HUB/MONGO SHELL/PYTHON
-----------------------------------------------------------

Search MongoDB using MongoHub for regex
{"url" : {"$regex" : ".*huffingtonpost.*"}}


Search MongoDB with MongoHub for text length
{"$where":"this.extracted_text && this.extracted_text.length > 500"}

Remove field from Mongodb in MongoHub
{"$unset": {"geodata":"1"}}

Field value in array - MONGO SHELL
db.user_history_items.find({"geodata.primaryCountries":{$in:["ES"]} })
MONGOHUB -- {"geodata.primaryCountries":{"$in":["ES"]} }

Exists in MongoHub: { "geodata.primaryCities": { "$exists": true } }
db.terra_incognita.user_history_items.find({ "geodata.primaryCities": { "$exists": true } }).sort({"lastVisitTime":-1}).skip(0).limit(10)

Exists in PyMongo: app.db_user_history_collection.find({ "geodata.primaryCities": { "$exists": True } }).sort([("lastVisitTime",-1)]).skip(0).limit(10)

Exists in Mongo Shell: Select user's last 10 history items that match to cities
db.terra_incognita.user_history_items.find({ "geodata.primaryCities": { "$exists": true } }, {_id:1,typedCount:1,title:1,url:1,lastVisitTime:1,geodata.primaryCities:1,visitCount:1}).sort({"lastVisitTime":-1}).skip(0).limit(10)

Delete db


Delete collection (use Mongo shell)
db.collection-name.drop()

Query for just countries, cities, etc
db.terra_incognita.user_history_items.find({"userID":"52dbeee6bd028634678cd069"}, {geodata.primaryCountries:1,geodata.primary.States:1,geodata.primaryCities.name:1}).sort({ "_id": 1}).skip(0).limit(30)

Add username field in MongoHub
db.terra_incognita.users.update({"_id": { "$oid" : "52dbeee6bd028634678cd069" } }, {"$set":{"username":"kanarinka"}}, false, false)

INTEGRATING WITH FLASK-LOGIN & PERSONA
-----------------------------------------------------------
http://stackoverflow.com/questions/21033870/how-to-run-flask-login-flask-browserid-and-flask-sqlalchemy-in-harmony

TO REDO 1000 CITIES FILE
-----------------------------------------------------------
DOwnload from Google spreadsheet
Import into Refine
Make sure to set char encoding to UTF-8!!!
Sort by continent_name, then by country_name then by city_name
Export>Templating, make "cities" the main array in the file
Open up file and replace nulls with 0's
Save it as variable in 1000cities.js file

CLEANING MEDIACLOUD SOURCES FILE WITH REGEXES
-----------------------------------------------------------

Regex for matching MediaCloud top level domain names
https?:((/.*?/(.*?)/)|.*\n)

Regex for newlines
^\n

Regex select trailing slash
/$

Regex remove www or something but only from beginning of line
^www\.

Regex match livejournal accounts
.*livejournal.com

Match short domains for manual deleting if bad data
^.{1,6}\n

Match things in parens
\(([^\)]+)\)


Converting mediacloud csv to tld whitelist (possibly write scripte for this later)
1. delete other columns in spreadsheet program
2. use above regex to select tlds
3. copy to new file
3.5 delete extra /n generated
4. manually delete bad uris
5. delete "#spider" from url
6. use Sublime "permute lines>unique" to remove dupes
7. Remove trailing slash from urls if they have one
8. remove http:// or https://
9. remove www. and blog. and blogs. and m. to keep the widest range of these sites (e.g. anything blah.nytimes.com would be included rather than just www.nytimes.com)
10. reorder alphabetically and then permute lines again
11. manually remove any long-looking or bad looking urls
12. remove all specific livejournal.com links and replace with top level 'livejournal.com' so any livejournal account will be counted as news
13. Match domains with length less than 3 chars and delete
14. Manually delete domains beginnig with xn
15. Manually delete anything that contains http: because it's definitely bad data at this point
16. Search for ':' and manually delete anything that looks like bad data
17. add theguardian.com
18. remove everything with GOOGLE in domain -- ^.*google\..*$
19. Remove facebook.com and twitter.com links

PYTHON PACKAGES INSTALL DIR ON MACOSX
-----------------------------------------------------------
/Library/Python/2.7/site-packages/

ACCESS TOKEN FOR BITLY API
-----------------------------------------------------------
curl -u "username:pwd" -X POST "https://api-ssl.bitly.com/oauth/access_token"

edit .profile or .bash-profile to set up BITLY_ACCESS_TOKEN=<YOUR TOKEN HERE>